{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This contains some of the initial code we used to compose our CNN for the MNIST database; once we completed and refined the CNN to an accuracy we were satisfied with we saved it into our working directory and uploaded it when we wanted to use it in our actual pipeline and for analysis; our accuracy / miscclassification analysis of our final CNN is at the bottom of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Format data for keras API; this data-preparation section advised by linked website\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "img_size = (28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-17b6bcf5639c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Use sequential model because based on research that is what seems like is the best for classification CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#Use sequential model because based on research that is what seems like is the best for classification CNN\n",
    "model = Sequential()\n",
    "\n",
    "#Perform initial convolution. Activation just linear. 3x3 conv-> valid padding would reduce size to 26x26\n",
    "#adding more filters takes longer, inc acc\n",
    "#Use 3x3 because research suggested 3x3 or 5x5 and our img size was only 28x28\n",
    "model.add(Conv2D(16, kernel_size=(3,3), padding=\"same\", input_shape=img_size))\n",
    "\n",
    "#Perform 2nd conv\n",
    "#Adding more layers takes longer, inc acc\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation = \"relu\"))\n",
    "#Pooling for feature control for not perfectly aligned data set\n",
    "#Use max pooling because that's what was recommended in Assignment 4 and articles suggested it\n",
    "#Use pooling to downsample more robustly than stride increase\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25)) #dropout for regularization; .25 kind of arbitrary as rate of disassoc.\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Flatten array for fully connected layers \n",
    "model.add(Flatten())\n",
    "\n",
    "#Do fc layer with 256 nodes; would do 14x14x64 but that had estimated training time of a day\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "#Adding another fully conn (like 128 or 64) doesnt help and makes training super long\n",
    "model.add(Dense(10,activation=\"softmax\"))\n",
    "\n",
    "#Use adam optimizer and sparse crossentropy loss function based on research. Cross-ent better for classification\n",
    "#Sparse does not require hot encoding, ie that we necessarily hardcode our 10 classes in memory\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Stop if doesnt imporove after 3 epochs\n",
    "checkImprovement = EarlyStopping(patience=3)\n",
    "\n",
    "#Data small enough so fit_generator not rqd\n",
    "model.fit(x=x_train,y=y_train, validation_split=.2, epochs=10, callbacks=[checkImprovement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use adam optimizer and sparse crossentropy loss function based on research. Cross-ent better for classification\n",
    "#Sparse does not require hot encoding, ie that we necessarily hardcode our 10 classes in memory\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Stop if doesnt imporove after 3 epochs\n",
    "checkImprovement = EarlyStopping(patience=3)\n",
    "\n",
    "#Data small enough so fit_generator not rqd\n",
    "model.fit(x=x_train,y=y_train, validation_split=.2, epochs=10, callbacks=[checkImprovement])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "image_index = 4444\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = cv2.imread(r'C:\\Users\\MattHetrick\\Documents\\Academics\\Fall 2019\\Cos 429\\Final-Project\\One.jpg')\n",
    "reg1 = cv2.imread(r'C:\\Users\\MattHetrick\\Documents\\Academics\\Fall 2019\\Cos 429\\Final-Project\\New1.jpg')\n",
    "flat2 = cv2.imread(r'C:\\Users\\MattHetrick\\Documents\\Academics\\Fall 2019\\Cos 429\\Final-Project\\Flat2.jpg')\n",
    "two = cv2.imread(r'C:\\Users\\MattHetrick\\Documents\\Academics\\Fall 2019\\Cos 429\\Final-Project\\2.jpg')\n",
    "seven = cv2.imread(r'C:\\Users\\MattHetrick\\Documents\\Academics\\Fall 2019\\Cos 429\\Final-Project\\7.jpg')\n",
    "eight = cv2.imread(r'C:\\Users\\MattHetrick\\Documents\\Academics\\Fall 2019\\Cos 429\\Final-Project\\8.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNumber(img):\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    squareGray = imgGray[1000:3500, 0:2500] #square size depends on img but worked for all sample images\n",
    "\n",
    "    resizedSquare = cv2.resize(squareGray, (28, 28), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    #format for keras\n",
    "    squareAsArray = resizedSquare.reshape(28, 28, 1)\n",
    "    squareAsArray = squareAsArray.astype('float32')\n",
    "\n",
    "    #make all black and white, black number\n",
    "    squareAsArray[squareAsArray <= 180.] = 255.\n",
    "    squareAsArray[squareAsArray < 255.] = 0.\n",
    "\n",
    "    #format for keras (again)\n",
    "    squareAsArray /= 255\n",
    "    predictionImage = squareAsArray.reshape(1, 28, 28, 1)\n",
    "    \n",
    "    #visualize image\n",
    "    #plt.imshow(predictionImage.reshape(28,28), cmap = \"Greys\")\n",
    "\n",
    "    pred = model.predict(predictionImage)\n",
    "    \n",
    "    return pred.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.2) /opt/concourse/worker/volumes/live/9523d527-1b9e-48e0-7ed0-a36adde286f0/volume/opencv-suite_1535558719691/work/modules/imgproc/src/color.hpp:253: error: (-215:Assertion failed) VScn::contains(scn) && VDcn::contains(dcn) && VDepth::contains(depth) in function 'CvtHelper'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-deb1f5763d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7f8772fff18f>\u001b[0m in \u001b[0;36mpredictNumber\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredictNumber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimgGray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msquareGray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgGray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#square size depends on img but worked for all sample images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mresizedSquare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquareGray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(3.4.2) /opt/concourse/worker/volumes/live/9523d527-1b9e-48e0-7ed0-a36adde286f0/volume/opencv-suite_1535558719691/work/modules/imgproc/src/color.hpp:253: error: (-215:Assertion failed) VScn::contains(scn) && VDcn::contains(dcn) && VDepth::contains(depth) in function 'CvtHelper'\n"
     ]
    }
   ],
   "source": [
    "predictNumber(two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input model and perform misclassification analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras import activations, initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"Model_3conv.h5\", custom_objects = {'softmax_v2': tf.nn.softmax})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 145us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03472121387043682, 0.9922000169754028]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(x_test) \n",
    "pred_class = np.argmax(predicted, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.absolute(y_test - pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified label = 7 || correct label =  9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num = 2582\n",
    "plt.imshow(x_test[num].reshape(28, 28),cmap='Greys')\n",
    "print(\"Misclassified label =\",pred_class[num], \"|| correct label = \",y_test[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "missclassified = []\n",
    "misclassified_diff = []\n",
    "\n",
    "for i in range(0,diff.size):\n",
    "    if (diff[i] > 0):\n",
    "        missclassified.append(i)\n",
    "        misclassified_diff.append(diff[i])\n",
    "        cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
